{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.568304   0.568283  ]\n",
      " [1.         0.310968   0.310956  ]\n",
      " [1.         0.103376   0.103373  ]\n",
      " [1.         0.0531882  0.053218  ]\n",
      " [1.         0.97006    0.970064  ]\n",
      " [1.         0.0941873  0.0941707 ]\n",
      " [1.         0.655902   0.655892  ]\n",
      " [1.         0.370821   0.370839  ]\n",
      " [1.         0.558482   0.558476  ]\n",
      " [1.         0.849389   0.849383  ]\n",
      " [1.         0.796038   0.796051  ]\n",
      " [1.         0.723246   0.723252  ]\n",
      " [1.         0.571236   0.571254  ]\n",
      " [1.         0.385144   0.38512   ]\n",
      " [1.         0.877176   0.877168  ]\n",
      " [1.         0.74655    0.746552  ]\n",
      " [1.         0.0676164  0.0676087 ]\n",
      " [1.         0.0412524  0.0412649 ]\n",
      " [1.         0.851637   0.851661  ]\n",
      " [1.         0.586989   0.58698   ]\n",
      " [1.         0.661014   0.660994  ]\n",
      " [1.         0.587988   0.587968  ]\n",
      " [1.         0.257615   0.257628  ]\n",
      " [1.         0.680505   0.680485  ]\n",
      " [1.         0.895242   0.895257  ]\n",
      " [1.         0.381124   0.381139  ]\n",
      " [1.         0.314332   0.31433   ]\n",
      " [1.         0.157744   0.157747  ]\n",
      " [1.         0.670923   0.670925  ]\n",
      " [1.         0.531716   0.531736  ]\n",
      " [1.         0.810956   0.810938  ]\n",
      " [1.         0.514937   0.51493   ]\n",
      " [1.         0.188567   0.188587  ]\n",
      " [1.         0.778528   0.778527  ]\n",
      " [1.         0.904966   0.904955  ]\n",
      " [1.         0.563699   0.563708  ]\n",
      " [1.         0.599768   0.59978   ]\n",
      " [1.         0.619909   0.619928  ]\n",
      " [1.         0.650556   0.650556  ]\n",
      " [1.         0.131949   0.131967  ]\n",
      " [1.         0.251546   0.251546  ]\n",
      " [1.         0.690874   0.690863  ]\n",
      " [1.         0.381249   0.381284  ]\n",
      " [1.         0.559231   0.559232  ]\n",
      " [1.         0.197361   0.197367  ]\n",
      " [1.         0.784776   0.784781  ]\n",
      " [1.         0.620494   0.620499  ]\n",
      " [1.         0.229646   0.229647  ]\n",
      " [1.         0.0891466  0.0891438 ]\n",
      " [1.         0.981857   0.981861  ]\n",
      " [1.         0.64711    0.647102  ]\n",
      " [1.         0.725596   0.725592  ]\n",
      " [1.         0.614771   0.614764  ]\n",
      " [1.         0.976315   0.976321  ]\n",
      " [1.         0.250716   0.250708  ]\n",
      " [1.         0.281071   0.281096  ]\n",
      " [1.         0.550196   0.550187  ]\n",
      " [1.         0.955756   0.955751  ]\n",
      " [1.         0.251821   0.251838  ]\n",
      " [1.         0.538196   0.538183  ]\n",
      " [1.         0.58285    0.582836  ]\n",
      " [1.         0.48367    0.48368   ]\n",
      " [1.         0.481451   0.481471  ]\n",
      " [1.         0.291576   0.291561  ]\n",
      " [1.         0.181592   0.181596  ]\n",
      " [1.         0.232746   0.232759  ]\n",
      " [1.         0.488322   0.488349  ]\n",
      " [1.         0.664499   0.664487  ]\n",
      " [1.         0.0420094  0.0420475 ]\n",
      " [1.         0.950521   0.950524  ]\n",
      " [1.         0.445707   0.445706  ]\n",
      " [1.         0.430385   0.430396  ]\n",
      " [1.         0.747574   0.747583  ]\n",
      " [1.         0.245047   0.245078  ]\n",
      " [1.         0.742838   0.742833  ]\n",
      " [1.         0.284625   0.284627  ]\n",
      " [1.         0.0613909  0.061374  ]\n",
      " [1.         0.612767   0.612754  ]\n",
      " [1.         0.378545   0.378555  ]\n",
      " [1.         0.818764   0.818763  ]\n",
      " [1.         0.0507026  0.0507136 ]\n",
      " [1.         0.882725   0.882731  ]\n",
      " [1.         0.0810847  0.0810796 ]\n",
      " [1.         0.836278   0.836279  ]\n",
      " [1.         0.696709   0.696695  ]\n",
      " [1.         0.603346   0.603334  ]\n",
      " [1.         0.513718   0.513712  ]\n",
      " [1.         0.247789   0.247802  ]\n",
      " [1.         0.704221   0.704213  ]\n",
      " [1.         0.546723   0.546724  ]\n",
      " [1.         0.881583   0.881592  ]\n",
      " [1.         0.13456    0.134545  ]\n",
      " [1.         0.86883    0.868815  ]\n",
      " [1.         0.980909   0.980887  ]\n",
      " [1.         0.369986   0.369986  ]\n",
      " [1.         0.194455   0.194457  ]\n",
      " [1.         0.483858   0.483875  ]\n",
      " [1.         0.43807    0.43808   ]\n",
      " [1.         0.159602   0.159592  ]\n",
      " [1.         0.923499   0.923504  ]\n",
      " [1.         0.419902   0.419906  ]\n",
      " [1.         0.659252   0.659271  ]\n",
      " [1.         0.419546   0.419546  ]\n",
      " [1.         0.935494   0.935512  ]\n",
      " [1.         0.712397   0.71239   ]\n",
      " [1.         0.952567   0.952549  ]\n",
      " [1.         0.915359   0.915379  ]\n",
      " [1.         0.182693   0.182675  ]\n",
      " [1.         0.668527   0.668522  ]\n",
      " [1.         0.0965221  0.0965266 ]\n",
      " [1.         0.984174   0.984197  ]\n",
      " [1.         0.7437     0.743702  ]\n",
      " [1.         0.213357   0.213341  ]\n",
      " [1.         0.617402   0.617386  ]\n",
      " [1.         0.335604   0.335604  ]\n",
      " [1.         0.632581   0.632597  ]\n",
      " [1.         0.515744   0.515757  ]\n",
      " [1.         0.786921   0.786912  ]\n",
      " [1.         0.502608   0.502599  ]\n",
      " [1.         0.164538   0.164537  ]\n",
      " [1.         0.507454   0.507469  ]\n",
      " [1.         0.822809   0.822806  ]\n",
      " [1.         0.42883    0.428821  ]\n",
      " [1.         0.157678   0.157693  ]\n",
      " [1.         0.674884   0.674896  ]\n",
      " [1.         0.276618   0.276622  ]\n",
      " [1.         0.374795   0.374795  ]\n",
      " [1.         0.396781   0.396815  ]\n",
      " [1.         0.132116   0.132101  ]\n",
      " [1.         0.966203   0.966249  ]\n",
      " [1.         0.961164   0.961159  ]\n",
      " [1.         0.0140044  0.014014  ]\n",
      " [1.         0.509361   0.509379  ]\n",
      " [1.         0.195082   0.195097  ]\n",
      " [1.         0.853012   0.853012  ]\n",
      " [1.         0.852883   0.852896  ]\n",
      " [1.         0.574279   0.574282  ]\n",
      " [1.         0.316965   0.316939  ]\n",
      " [1.         0.386753   0.386761  ]\n",
      " [1.         0.764792   0.764815  ]\n",
      " [1.         0.680442   0.680428  ]\n",
      " [1.         0.125299   0.125304  ]\n",
      " [1.         0.619824   0.619818  ]\n",
      " [1.         0.687672   0.687662  ]\n",
      " [1.         0.760271   0.760289  ]\n",
      " [1.         0.227148   0.22713   ]\n",
      " [1.         0.224288   0.224295  ]\n",
      " [1.         0.0150326  0.0150352 ]\n",
      " [1.         0.585322   0.585314  ]\n",
      " [1.         0.732755   0.732777  ]\n",
      " [1.         0.864553   0.864569  ]\n",
      " [1.         0.0788415  0.0788569 ]\n",
      " [1.         0.4326     0.432602  ]\n",
      " [1.         0.804816   0.804801  ]\n",
      " [1.         0.50957    0.509589  ]\n",
      " [1.         0.405003   0.404988  ]\n",
      " [1.         0.465702   0.465691  ]\n",
      " [1.         0.368576   0.368574  ]\n",
      " [1.         0.56202    0.562033  ]\n",
      " [1.         0.552361   0.552356  ]\n",
      " [1.         0.18263    0.182606  ]\n",
      " [1.         0.672912   0.672906  ]\n",
      " [1.         0.642397   0.642413  ]\n",
      " [1.         0.816308   0.816316  ]\n",
      " [1.         0.264986   0.264978  ]\n",
      " [1.         0.799168   0.799179  ]\n",
      " [1.         0.311442   0.311432  ]\n",
      " [1.         0.715291   0.715278  ]\n",
      " [1.         0.913262   0.913265  ]\n",
      " [1.         0.703566   0.70358   ]\n",
      " [1.         0.0868818  0.0868856 ]\n",
      " [1.         0.507828   0.507835  ]\n",
      " [1.         0.77619    0.776196  ]\n",
      " [1.         0.503254   0.503257  ]\n",
      " [1.         0.0585257  0.0585251 ]\n",
      " [1.         0.668003   0.667995  ]\n",
      " [1.         0.409675   0.409686  ]\n",
      " [1.         0.00104673 0.00105247]\n",
      " [1.         0.6743     0.674268  ]\n",
      " [1.         0.461383   0.461378  ]\n",
      " [1.         0.957667   0.957677  ]\n",
      " [1.         0.386593   0.386566  ]\n",
      " [1.         0.260177   0.260171  ]\n",
      " [1.         0.208071   0.208076  ]\n",
      " [1.         0.634661   0.634646  ]\n",
      " [1.         0.354351   0.354351  ]\n",
      " [1.         0.135384   0.135381  ]\n",
      " [1.         0.216718   0.216748  ]\n",
      " [1.         0.606084   0.606096  ]\n",
      " [1.         0.443809   0.443801  ]\n",
      " [1.         0.480428   0.480418  ]\n",
      " [1.         0.886987   0.886995  ]\n",
      " [1.         0.0126171  0.012603  ]\n",
      " [1.         0.578502   0.578495  ]\n",
      " [1.         0.0664441  0.0664438 ]\n",
      " [1.         0.292442   0.292432  ]\n",
      " [1.         0.487013   0.487008  ]\n",
      " [1.         0.176237   0.176234  ]\n",
      " [1.         0.496052   0.496044  ]\n",
      " [1.         0.62186    0.621853  ]] 200\n",
      "[[1.       0.731977 0.731978]\n",
      " [1.       0.987864 0.987864]\n",
      " [1.       0.402041 0.402048]\n",
      " ...\n",
      " [1.       0.786785 0.786803]\n",
      " [1.       0.545345 0.545348]\n",
      " [1.       0.709711 0.709694]] 1000\n"
     ]
    }
   ],
   "source": [
    "#Q13.\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def download_data(url,filename):    #download file as data\n",
    "    result = requests.get(url)\n",
    "    result.raise_for_status()\n",
    "    with open(filename,'wb') as FILE:\n",
    "        for chunk in result.iter_content(102400):\n",
    "            FILE.write(chunk)\n",
    "            \n",
    "url01 = 'https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw4_train.dat'\n",
    "url02 = 'https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw4_test.dat'\n",
    "download_data(url01, 'HW4_training_data.txt')\n",
    "download_data(url02, 'HW4_test_data.txt')\n",
    "    \n",
    "    \n",
    "def readout(filename):\n",
    "    with open(filename,'r') as FILE:\n",
    "        n, Dx, Dy = 0 ,[], []\n",
    "        \n",
    "        for chunk in FILE:\n",
    "            X= chunk.split()   #split the line into a list of string\n",
    "            X= [ float(X[j]) for j in range(len(X)) ]  #convert the string to numbers   \n",
    "            X.insert(0,1)                #insert a constant as bias or as threshold\n",
    "            Dx.append(X[0:len(X)-1])     #gather all data ## index len(X)-1 will not be included here\n",
    "            Dy.append(X[len(X)-1])       #index len(X)-1 only\n",
    "            n=n+1\n",
    "        x = np.array(Dx)  #change list X into array x, and take transpose by .T\n",
    "        y = np.array(Dy)\n",
    "    print(x,n)            \n",
    "    return x,y,n        \n",
    "\n",
    "trainx, trainy, trainn = readout('HW4_training_data.txt')\n",
    "testx, testy, testn    = readout('HW4_test_data.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\nabla E_{in}(W_{REG}) + \\frac{2\\lambda}{N}W_{REG} = 0   $$ $$W_{REG} \\leftarrow (Z^TZ + \\lambda I)^{-1}Z^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(w,datax,datay,datan):\n",
    "    err = []\n",
    "#   yhat = [np.dot(w,datax[i]) for i in range(datan)]  #\n",
    "    yhat = datax.dot(w)\n",
    "\n",
    "    for a,b in zip(yhat,datay):\n",
    "        if a*b<=0:\n",
    "            err.append(1)\n",
    "        else:\n",
    "            err.append(0)\n",
    "    error = sum(err) /datan\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 0.045\n"
     ]
    }
   ],
   "source": [
    "#Q13.\n",
    "lamb = 10\n",
    "\n",
    "wreg = np.linalg.inv(np.dot(trainx.T,trainx) + lamb*np.identity(len(trainx[0]))).dot(trainx.T).dot(trainy)\n",
    "\n",
    "\n",
    "Ein = error(wreg,trainx,trainy,trainn)\n",
    "Eout = error(wreg,testx,testy,testn)\n",
    "print(Ein,Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.24 0.261\n",
      "10 0.05 0.045\n",
      "1 0.035 0.02\n",
      "1 0.035 0.02\n",
      "0.01 0.03 0.016\n",
      "0.01 0.03 0.016\n",
      "0.01 0.03 0.016\n",
      "0.01 0.03 0.016\n",
      "0.01 0.03 0.016\n",
      "0.01 0.03 0.016\n",
      "1e-08 0.015 0.02\n",
      "1e-08 0.015 0.02\n",
      "1e-08 0.015 0.02\n"
     ]
    }
   ],
   "source": [
    "#Q14.\n",
    "Ein = 1\n",
    "Eout = 1\n",
    "for i in range(2,-11,-1):\n",
    "    alpha = 10**i\n",
    "    wreg = np.linalg.inv(np.dot(trainx.T,trainx) + alpha*np.identity(len(trainx[0]))).dot(trainx.T).dot(trainy)\n",
    "    \n",
    "    ein = error(wreg,trainx,trainy,trainn)\n",
    "    eout = error(wreg,testx,testy,testn)\n",
    "    \n",
    "    if ein < Ein:  #leave the biggest lamb with the min Ein\n",
    "        Ein = ein\n",
    "        Eout = eout\n",
    "        lamb = alpha\n",
    "        \n",
    "    print(lamb, Ein, Eout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.24 0.261\n",
      "10 0.05 0.045\n",
      "1 0.035 0.02\n",
      "0.1 0.035 0.016\n",
      "0.1 0.035 0.016\n",
      "0.1 0.035 0.016\n",
      "0.1 0.035 0.016\n",
      "0.1 0.035 0.016\n",
      "0.1 0.035 0.016\n",
      "1e-07 0.03 0.015\n",
      "1e-07 0.03 0.015\n",
      "1e-07 0.03 0.015\n",
      "1e-07 0.03 0.015\n"
     ]
    }
   ],
   "source": [
    "#Q15.\n",
    "Ein = 1\n",
    "Eout = 1\n",
    "for i in range(2,-11,-1):\n",
    "    alpha = 10**i\n",
    "    wreg = np.linalg.inv(np.dot(trainx.T,trainx) + alpha*np.identity(len(trainx[0]))).dot(trainx.T).dot(trainy)\n",
    "    \n",
    "    ein = error(wreg,trainx,trainy,trainn)\n",
    "    eout = error(wreg,testx,testy,testn)\n",
    "    \n",
    "    if eout < Eout: #leave the biggest lamb with the min Eout\n",
    "        Ein = ein\n",
    "        Eout = eout\n",
    "        lamb = alpha\n",
    "        \n",
    "    print(lamb, Ein, Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.3416666666666667 0.4125 0.414\n",
      "10 0.075 0.125 0.08\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1e-08 0.0 0.05 0.025\n",
      "1e-08 0.0 0.05 0.025\n",
      "1e-08 0.0 0.05 0.025\n"
     ]
    }
   ],
   "source": [
    "#Q16.\n",
    "k = 80                  #validation size\n",
    "j = trainn-k            #train data size\n",
    "\n",
    "Dtrainx, Dtrainy, Dtrainn = trainx[0:j], trainy[0:j], j \n",
    "Dvalx  , Dvaly  , Dvaln   = trainx[j:,], trainy[j:] , k\n",
    "Etrain, Eval, Eout = 1, 1, 1\n",
    "\n",
    "\n",
    "for i in range(2,-11,-1):\n",
    "    alpha = 10**i\n",
    "    wreg = np.linalg.inv(np.dot(Dtrainx.T,Dtrainx) + alpha*np.identity(len(Dtrainx[0]))).dot(Dtrainx.T).dot(Dtrainy)\n",
    "    \n",
    "    etrain = error(wreg,Dtrainx,Dtrainy,Dtrainn)\n",
    "    evali = error(wreg,Dvalx,Dvaly,Dvaln)\n",
    "    eout = error(wreg,testx,testy,testn)\n",
    "    \n",
    "    if etrain < Etrain: #leave the biggest lamb with the min Etrain\n",
    "        Etrain = etrain\n",
    "        Eval = evali\n",
    "        Eout = eout\n",
    "        lamb = alpha\n",
    "        \n",
    "    print(lamb, Etrain, Eval, Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.3416666666666667 0.4125 0.414\n",
      "10 0.075 0.125 0.08\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n",
      "1 0.03333333333333333 0.0375 0.028\n"
     ]
    }
   ],
   "source": [
    "#Q17.\n",
    "k = 80                  #validation size\n",
    "j = trainn-k            #train data size\n",
    "\n",
    "Dtrainx, Dtrainy, Dtrainn = trainx[0:j], trainy[0:j], j \n",
    "Dvalx  , Dvaly  , Dvaln   = trainx[j:,], trainy[j:] , k\n",
    "Etrain, Eval, Eout = 1, 1, 1\n",
    "\n",
    "\n",
    "for i in range(2,-11,-1):\n",
    "    alpha = 10**i\n",
    "    wreg = np.linalg.inv(np.dot(Dtrainx.T,Dtrainx) + alpha*np.identity(len(Dtrainx[0]))).dot(Dtrainx.T).dot(Dtrainy)\n",
    "    \n",
    "    etrain = error(wreg,Dtrainx,Dtrainy,Dtrainn)\n",
    "    evali = error(wreg,Dvalx,Dvaly,Dvaln)\n",
    "    eout = error(wreg,testx,testy,testn)\n",
    "    \n",
    "    if evali < Eval: #leave the biggest lamb with the min Etrain\n",
    "        Etrain = etrain\n",
    "        Eval = evali\n",
    "        Eout = eout\n",
    "        lamb = alpha\n",
    "        \n",
    "    print(lamb, Etrain,Eval, Eout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035 0.02\n"
     ]
    }
   ],
   "source": [
    "#Q18.\n",
    "\n",
    "wreg = np.linalg.inv(np.dot(trainx.T,trainx) + lamb*np.identity(len(trainx[0]))).dot(trainx.T).dot(trainy)\n",
    "\n",
    "Ein = error(wreg,trainx,trainy,trainn)\n",
    "Eout = error(wreg,testx,testy,testn)\n",
    "print(Ein,Eout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V-fold cross-validation:random-partition of D to V equal parts,  \n",
    "take V-1 for training and 1 for validation orderly.  \n",
    "$$E_{CV}(H,A) = \\frac{1}{V}\\Sigma^V_{v=1}(g^-_V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.24 0.29000000000000004 0.261\n",
      "10 0.05 0.06 0.045\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1 0.035 0.034999999999999996 0.02\n",
      "1e-08 0.015 0.03 0.02\n",
      "1e-08 0.015 0.03 0.02\n",
      "1e-08 0.015 0.03 0.02\n"
     ]
    }
   ],
   "source": [
    "#Q19 & Q20.\n",
    "V = 5\n",
    "k = int(trainn/V)                  #size of each fold\n",
    "E=0\n",
    "\n",
    "\n",
    "Etrain, Eval, Eout, Ecv = 1, 1, 1, 1\n",
    "\n",
    "\n",
    "for i in range(2,-11,-1):\n",
    "    alpha = 10**i\n",
    "    E=0\n",
    "    for j in range(5):\n",
    "        \n",
    "        tempx ,tempy = trainx, trainy\n",
    "        Dtrainx, Dtrainy, Dtrainn = np.delete(tempx,[range(j*k,(j+1)*k)],axis=0), np.delete(tempy,[range(j*k,(j+1)*k)],axis=0), (V-1)*k\n",
    "        Dvalx  , Dvaly  , Dvaln   = trainx[j*k:(j+1)*k], trainy[j*k:(j+1)*k], k\n",
    "        \n",
    "        wreg = np.linalg.inv(np.dot(Dtrainx.T,Dtrainx) + alpha*np.identity(len(Dtrainx[0]))).dot(Dtrainx.T).dot(Dtrainy)\n",
    "\n",
    "        #etrain = error(wreg,Dtrainx,Dtrainy,Dtrainn)\n",
    "        evali = error(wreg,Dvalx,Dvaly,Dvaln)\n",
    "        #eout = error(wreg,testx,testy,testn)\n",
    "        E = E+evali\n",
    "    ecv = E/V   #Q19.\n",
    "    \n",
    "    if ecv < Ecv: #leave the biggest lamb with the min Etrain   #Q20.\n",
    "        lamb = alpha\n",
    "        wreg = np.linalg.inv(np.dot(trainx.T,trainx) + alpha*np.identity(len(trainx[0]))).dot(trainx.T).dot(trainy)\n",
    "        Etrain = error(wreg,trainx,trainy,trainn)\n",
    "        Ecv = ecv\n",
    "        Eout = error(wreg,testx,testy,testn)\n",
    "\n",
    "\n",
    "    print(lamb, Etrain,Ecv, Eout)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B = np.array([[4,5,6],[7,8,9],[10,11,12]])\n",
    "np.append(A,np.array([[1,2,3]]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempx ,tempy = trainx, trainy\n",
    "j=3\n",
    "V = 5\n",
    "k = int(trainn/V)                  \n",
    "Dtrainx, Dtrainy, Dtrainn = np.delete(tempx,[range(j*k,(j+1)*k)],axis=0), np.delete(tempy,[range(j*k,(j+1)*k)],axis=0) , (V-1)*k\n",
    "Dvalx  , Dvaly  , Dvaln   = trainx[j*k:(j+1)*k], trainy[j*k:(j+1)*k], k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dtrainx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
